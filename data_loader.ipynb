{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9df2cc81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "89336606",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 4358/4358 [00:00<00:00, 5356.56 examples/s]\n",
      "Map: 100%|██████████| 36718/36718 [00:05<00:00, 6537.05 examples/s]\n",
      "Map: 100%|██████████| 3760/3760 [00:01<00:00, 3490.53 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    test: Dataset({\n",
       "        features: ['input_ids'],\n",
       "        num_rows: 4358\n",
       "    })\n",
       "    train: Dataset({\n",
       "        features: ['input_ids'],\n",
       "        num_rows: 36718\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['input_ids'],\n",
       "        num_rows: 3760\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\")\n",
    "\n",
    "enc = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "def tokenize(example):\n",
    "    return {\"input_ids\": enc.encode(example[\"text\"], allowed_special=set())}\n",
    "\n",
    "tokenized = dataset.map(tokenize, remove_columns=[\"text\"])\n",
    "tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "84de2fc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (9343, 256)\n",
      "Val: (965, 256)\n"
     ]
    }
   ],
   "source": [
    "def create_chunks(dataset_split, chunk_size=256):\n",
    "    all_tokens = []\n",
    "    for example in dataset_split:\n",
    "        all_tokens.extend(example[\"input_ids\"])\n",
    "\n",
    "    all_tokens = np.array(all_tokens)\n",
    "\n",
    "    n_chunks = len(all_tokens) // chunk_size\n",
    "    all_tokens = all_tokens[:n_chunks * chunk_size]\n",
    "    chunks = all_tokens.reshape(-1, chunk_size)\n",
    "\n",
    "    return chunks\n",
    "\n",
    "train_data = create_chunks(tokenized[\"train\"], chunk_size=256)\n",
    "test_data = create_chunks(tokenized[\"test\"], chunk_size=256)\n",
    "val_data = create_chunks(tokenized[\"validation\"], chunk_size=256)\n",
    "\n",
    "print(f\"Train: {train_data.shape}\")  # (num_sequences, 256)\n",
    "print(f\"Val: {val_data.shape}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
